Feature selection is a critical step in data preprocessing that helps improve model performance and reduce computational costs. Here are several popular methods for feature selection:

### 1. **Filter Methods**

Filter methods select features based on statistical tests and metrics.

- **Correlation**: Remove features that have high correlation with other features. For example, you might drop one of two highly correlated features.

  ```python
  import numpy as np

  corr_matrix = X.corr().abs()
  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
  to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]
  X_reduced = X.drop(columns=to_drop)
  ```

- **Statistical Tests**: Use statistical tests such as the Chi-square test for categorical features or ANOVA for numerical features.

  ```python
  from sklearn.feature_selection import chi2, SelectKBest

  chi2_selector = SelectKBest(score_func=chi2, k='all')
  X_chi2 = chi2_selector.fit_transform(X, y)
  ```

- **Mutual Information**: Evaluate mutual information between features and the target variable.

  ```python
  from sklearn.feature_selection import mutual_info_classif

  mi = mutual_info_classif(X, y)
  mi_df = pd.DataFrame({'Feature': X.columns, 'MI': mi})
  top_features = mi_df.sort_values(by='MI', ascending=False).head(10)
  ```

### 2. **Wrapper Methods**

Wrapper methods evaluate the performance of the model for different subsets of features.

- **Stepwise Selection**: Iteratively add or remove features and evaluate the model.

  ```python
  from sklearn.feature_selection import RFE
  from sklearn.linear_model import LogisticRegression

  model = LogisticRegression()
  rfe = RFE(model, n_features_to_select=5)
  fit = rfe.fit(X, y)
  print("Selected Features: %s" % fit.support_)
  print("Feature Ranking: %s" % fit.ranking_)
  ```

- **Tree-based Methods**: Use tree-based models like Random Forest or Gradient Boosting to select features.

  ```python
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.feature_selection import SelectFromModel

  model = RandomForestClassifier()
  model.fit(X, y)
  selector = SelectFromModel(model, threshold='mean', prefit=True)
  X_reduced = selector.transform(X)
  ```

### 3. **Embedded Methods**

Embedded methods combine model training and feature selection.

- **L1 Regularization**: Use L1 regularization for feature selection in models like Logistic Regression.

  ```python
  from sklearn.linear_model import LogisticRegression

  model = LogisticRegression(penalty='l1', solver='saga')
  model.fit(X, y)
  selected_features = X.columns[model.coef_.flatten() != 0]
  ```

- **Lasso (Least Absolute Shrinkage and Selection Operator)**: Use Lasso for feature selection in linear models.

  ```python
  from sklearn.linear_model import Lasso

  model = Lasso(alpha=0.01)
  model.fit(X, y)
  selected_features = X.columns[model.coef_ != 0]
  ```

### 4. **Recursive Feature Elimination (RFE)**

This method uses a model to assess the importance of features and recursively removes the least important ones.

```python
from sklearn.feature_selection import RFE
from sklearn.svm import SVR

model = SVR(kernel="linear")
rfe = RFE(model, n_features_to_select=5)
fit = rfe.fit(X, y)
selected_features = X.columns[fit.support_]
```

### 5. **Model Performance Comparison**

Use different feature selection methods and evaluate their impact on model performance, for example, through cross-validation.

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
scores_before = cross_val_score(model, X, y, cv=5)

# Apply feature selection method
X_reduced = ...  # your feature selection method
scores_after = cross_val_score(model, X_reduced, y, cv=5)

print("Score before feature selection:", scores_before.mean())
print("Score after feature selection:", scores_after.mean())
```

Choosing the right feature selection method depends on your dataset, task, and model type. Experiment with different approaches and assess their impact on model quality.
